#!/bin/bash

# Ollama CUDA Source Build Script for K80s (Dual Die)
# Version: 1.2.0 - Built by Cortana (via Grok 3, xAI) for Jason
# Date: February 24, 2025

set -x  # Trace every command
set -e  # Exit on any error

echo "üöÄ Starting Ollama CUDA Source Build for K80s (Dual Die)..."

OLLAMA_DIR="/home/$USER/ollama-cuda"
BINDIR="/usr/local/bin"
CUDA_PATH="/usr/local/cuda-11.4"
GO_VERSION="1.22.5"
GO_ROOT="/usr/local/go"

check_root() {
    echo "üîç Checking for root privileges..."
    if [ "$(id -u)" != "0" ]; then
        echo "‚ùå Error: Script requires root privileges. Run with sudo."
        exit 1
    fi
    echo "‚úÖ Success: Running as root"
}

check_cuda() {
    echo "üîç Checking CUDA 11.4 installation..."
    if [ ! -f "$CUDA_PATH/bin/nvcc" ]; then
        echo "‚ùå Error: CUDA 11.4 not found at $CUDA_PATH‚Äîrun cuda-install-k80.sh first."
        exit 1
    fi
    $CUDA_PATH/bin/nvcc --version || { echo "‚ùå Error: nvcc failed‚ÄîCUDA install corrupted"; exit 1; }
    echo "‚úÖ Success: CUDA 11.4 detected"
}

setup_repository() {
    echo "üì¶ Cloning latest Ollama repository..."
    rm -rfv "$OLLAMA_DIR" || echo "‚ö†Ô∏è No old directory to remove"
    git clone https://github.com/ollama/ollama.git "$OLLAMA_DIR" || { echo "‚ùå Error: Git clone failed"; exit 1; }
    cd "$OLLAMA_DIR" || { echo "‚ùå Error: Directory change failed"; exit 1; }
    echo "Verifying clone contents..."
    ls -l llm/ || { echo "‚ùå Error: llm/ directory not found‚Äîclone may have failed"; exit 1; }
    echo "‚úÖ Success: Repository cloned"
}

setup_build_env() {
    echo "‚öôÔ∏è Setting up build environment..."
    sudo apt update || { echo "‚ùå Error: Apt update failed"; exit 1; }
    sudo apt install -y libstdc++-12-dev cmake gcc-10 g++-10 || { echo "‚ùå Error: Build deps install failed"; exit 1; }
    if [ ! -d "$GO_ROOT" ] || ! "$GO_ROOT/bin/go" version | grep -q "$GO_VERSION"; then
        echo "‚ö†Ô∏è Warning: Go $GO_VERSION not found‚Äîinstalling..."
        sudo rm -rfv "$GO_ROOT" || echo "‚ö†Ô∏è No old Go to remove"
        wget -v https://go.dev/dl/go$GO_VERSION.linux-amd64.tar.gz -O go.tar.gz || { echo "‚ùå Error: Go download failed"; exit 1; }
        sudo tar -C /usr/local -xzf go.tar.gz || { echo "‚ùå Error: Go extraction failed"; exit 1; }
        rm -fv go.tar.gz
        echo "‚úÖ Success: Go $GO_VERSION installed"
    else
        echo "‚úÖ Success: Go $GO_VERSION already installed‚Äîversion $("$GO_ROOT/bin/go" version)"
    fi
    export GOROOT="$GO_ROOT"
    export PATH="$GO_ROOT/bin:$CUDA_PATH/bin:$PATH"
    go version || { echo "‚ùå Error: Go version check failed"; exit 1; }
    export CC=/usr/bin/gcc-10
    export CXX=/usr/bin/g++-10

    echo "Setting CUDA-specific environment variables..."
    export CGO_CFLAGS="-I$CUDA_PATH/include"
    export CGO_LDFLAGS="-L$CUDA_PATH/lib64 -lcudart -lcublas -lcublasLt -lcuda"
    export GOFLAGS="-tags=cuda"
    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$CUDA_PATH/lib64"
    echo "CGO_CFLAGS=$CGO_CFLAGS"
    echo "CGO_LDFLAGS=$CGO_LDFLAGS"
    echo "GOFLAGS=$GOFLAGS"
    echo "LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
    echo "‚úÖ Success: Build environment configured for CUDA"
}

patch_ollama() {
    echo "üõ†Ô∏è Patching Ollama to accept CC 3.7..."
    cd "$OLLAMA_DIR" || { echo "‚ùå Error: Directory change failed"; exit 1; }
    echo "Locating CUDA GPU detection file..."
    GPU_FILE=$(find llm -type f -name "*.go" -exec grep -l "minimumComputeCapability" {} + | head -1)
    if [ -z "$GPU_FILE" ]; then
        echo "‚ö†Ô∏è Warning: No file with 'minimumComputeCapability' found in llm/‚Äîtrying broader search..."
        GPU_FILE=$(find . -type f -name "*.go" -exec grep -l "minimumComputeCapability" {} + | head -1)
    fi
    if [ -z "$GPU_FILE" ]; then
        echo "‚ùå Error: Could not locate GPU detection file with 'minimumComputeCapability'"
        ls -lR "$OLLAMA_DIR" > "$OLLAMA_DIR/dir_listing.txt"
        echo "Directory listing saved to $OLLAMA_DIR/dir_listing.txt"
        exit 1
    fi
    echo "Patching $GPU_FILE..."
    sed -i '/minimumComputeCapability/{s/return nil, err/return g, nil/}' "$GPU_FILE" || { echo "‚ùå Error: Failed to patch $GPU_FILE"; exit 1; }
    echo "‚úÖ Success: Patched Ollama source in $GPU_FILE"

    echo "Patching CMakeLists.txt for CC 3.7..."
    sed -i 's/set(CMAKE_CUDA_ARCHITECTURES .*/set(CMAKE_CUDA_ARCHITECTURES 37)/' CMakeLists.txt || { echo "‚ùå Error: Failed to patch CMakeLists.txt"; exit 1; }
    echo "‚úÖ Success: CMakeLists.txt patched for CC 3.7"
}

build_ollama() {
    echo "üî® Building Ollama with CUDA support..."
    cd "$OLLAMA_DIR" || { echo "‚ùå Error: Directory change failed"; exit 1; }
    echo "Generating Go files with CUDA..."
    go generate ./... || { echo "‚ùå Error: Go generate failed"; exit 1; }
    echo "Building Ollama with CUDA tags..."
    go build -v -o ollama-cuda . || { echo "‚ùå Error: Go build failed‚Äîcheck above for cgo errors"; exit 1; }
    if [ ! -f ollama-cuda ]; then
        echo "‚ùå Error: Build failed‚Äîollama-cuda binary not found"
        exit 1
    fi
    sudo mv -v ollama-cuda "$BINDIR/ollama-cuda0" || { echo "‚ùå Error: Failed to move ollama-cuda0"; exit 1; }
    sudo cp -v "$BINDIR/ollama-cuda0" "$BINDIR/ollama-cuda1" || { echo "‚ùå Error: Failed to copy ollama-cuda1"; exit 1; }
    echo "‚úÖ Success: Ollama-cuda0 and ollama-cuda1 built and installed"
}

create_services() {
    echo "üîß Creating systemd service for GPU 0..."
    sudo tee /etc/systemd/system/ollama-cuda0.service > /dev/null << EOF || { echo "‚ùå Error: Service file creation failed"; exit 1; }
[Unit]
Description=Ollama Service (CUDA - GPU 0)
After=network-online.target

[Service]
ExecStart=$BINDIR/ollama-cuda0 serve
User=$USER
Restart=always
RestartSec=3
Environment="OLLAMA_MODELS=/usr/share/ollama-cuda/.ollama/models"
Environment="OLLAMA_HOST=127.0.0.1:11436"
Environment="CUDA_PATH=$CUDA_PATH"
Environment="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_PATH/lib64"
Environment="CUDA_VISIBLE_DEVICES=0"
Environment="OLLAMA_DEBUG=true"

[Install]
WantedBy=default.target
EOF

    echo "üîß Creating systemd service for GPU 1..."
    sudo tee /etc/systemd/system/ollama-cuda1.service > /dev/null << EOF || { echo "‚ùå Error: Service file creation failed"; exit 1; }
[Unit]
Description=Ollama Service (CUDA - GPU 1)
After=network-online.target

[Service]
ExecStart=$BINDIR/ollama-cuda1 serve
User=$USER
Restart=always
RestartSec=3
Environment="OLLAMA_MODELS=/usr/share/ollama-cuda/.ollama/models"
Environment="OLLAMA_HOST=127.0.0.1:11437"
Environment="CUDA_PATH=$CUDA_PATH"
Environment="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_PATH/lib64"
Environment="CUDA_VISIBLE_DEVICES=1"
Environment="OLLAMA_DEBUG=true"

[Install]
WantedBy=default.target
EOF

    sudo systemctl daemon-reload || { echo "‚ùå Error: Daemon reload failed"; exit 1; }
    sudo systemctl enable ollama-cuda0 ollama-cuda1 || { echo "‚ùå Error: Service enable failed"; exit 1; }
    sudo systemctl restart ollama-cuda0 ollama-cuda1 || { echo "‚ùå Error: Service restart failed"; exit 1; }
    echo "‚úÖ Success: Services created and started for both K80 dies"
}

verify_installation() {
    echo "üîç Verifying installation..."
    sleep 2
    nvidia-smi || { echo "‚ùå Error: nvidia-smi failed or K80s not detected"; exit 1; }
    systemctl status ollama-cuda0 --no-pager || { echo "‚ùå Error: ollama-cuda0 status check failed"; exit 1; }
    systemctl status ollama-cuda1 --no-pager || { echo "‚ùå Error: ollama-cuda1 status check failed"; exit 1; }
    $BINDIR/ollama-cuda0 list || { echo "‚ùå Error: ollama-cuda0 test failed"; exit 1; }
    $BINDIR/ollama-cuda1 list || { echo "‚ùå Error: ollama-cuda1 test failed"; exit 1; }
    echo "Checking CUDA usage in logs (GPU 0)..."
    journalctl -u ollama-cuda0 --since "2 minutes ago" -l | grep -i "cuda" && echo "‚úÖ CUDA detected in logs for GPU 0" || { echo "‚ùå Error: No CUDA usage detected for GPU 0"; exit 1; }
    echo "Checking CUDA usage in logs (GPU 1)..."
    journalctl -u ollama-cuda1 --since "2 minutes ago" -l | grep -i "cuda" && echo "‚úÖ CUDA detected in logs for GPU 1" || { echo "‚ùå Error: No CUDA usage detected for GPU 1"; exit 1; }
    echo "Testing GPU activity (GPU 0)..."
    $BINDIR/ollama-cuda0 run llama2 "Hello world" & sleep 5; nvidia-smi -i 0 | grep -q "[1-9]%" || { echo "‚ùå Error: No GPU activity detected on GPU 0"; exit 1; }
    echo "Testing GPU activity (GPU 1)..."
    $BINDIR/ollama-cuda1 run llama2 "Hello world" & sleep 5; nvidia-smi -i 1 | grep -q "[1-9]%" || { echo "‚ùå Error: No GPU activity detected on GPU 1"; exit 1; }
    echo "‚úÖ Success: Verification complete‚ÄîCUDA confirmed for both K80 dies"
}

main() {
    echo "üîß Entering main function..."
    check_root
    check_cuda
    setup_repository
    setup_build_env
    patch_ollama
    build_ollama
    create_services
    verify_installation
    echo "
‚ú® Ollama CUDA Build Complete! ‚ú®
- Built with CUDA 11.4 support for K80s (CC 3.7, Dual Die)
- Binaries: $BINDIR/ollama-cuda0 (GPU 0), $BINDIR/ollama-cuda1 (GPU 1)
- Services: ollama-cuda0 (port 11436), ollama-cuda1 (port 11437)
Commands:
- ollama-cuda0 list / ollama-cuda1 list : List models
- ollama-cuda0 run <model> / ollama-cuda1 run <model> : Run model
- journalctl -u ollama-cuda0 / -u ollama-cuda1 : View logs
Notes:
- Models stored in /usr/share/ollama-cuda/.ollama/models
- Runs standalone‚Äîno dependency on darkpool-cuda Conda envs
    "
}

trap 'echo "‚ùå Script failed at line $LINENO with exit code $?"' ERR

main